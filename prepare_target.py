import torch
import esm
import requests
import os
from tqdm import tqdm

# --- é…ç½® ---
SAVE_PATH = ".cache/torch/checkpoints/target_membrane_embedding.pt"
MODEL_NAME = "esm2_t33_650M_UR50D" # 650M å‚æ•°ç‰ˆï¼Œ4090 è·‘å¾—é£å¿«
NUM_SEQUENCES = 50                 # æ‹¿ 50 æ¡åšå¹³å‡è¶³å¤Ÿäº†
MAX_LENGTH = 1024                  # æˆªæ–­è¿‡é•¿çš„åºåˆ—é˜²æ­¢æ˜¾å­˜çˆ†ç‚¸

def fetch_membrane_proteins(num=50):
    """
    ä» UniProt API è‡ªåŠ¨ä¸‹è½½è†œè›‹ç™½åºåˆ—
    å…³é”®è¯: "transmembrane" AND "reviewed:true" (é«˜è´¨é‡)
    """
    print(f"ğŸŒ æ­£åœ¨ä» UniProt ä¸‹è½½ {num} æ¡è†œè›‹ç™½åºåˆ—...")
    url = "https://rest.uniprot.org/uniprotkb/search"
    params = {
        "query": "(keyword:\"Transmembrane [KW-0812]\") AND (reviewed:true)",
        "format": "fasta",
        "size": num
    }
    
    response = requests.get(url, params=params)
    response.raise_for_status()
    
    # ç®€å•çš„ FASTA è§£æ
    sequences = []
    lines = response.text.strip().split('\n')
    current_seq = ""
    current_header = ""
    
    for line in lines:
        if line.startswith(">"):
            if current_seq and len(current_seq) < MAX_LENGTH:
                sequences.append((current_header, current_seq))
            current_header = line
            current_seq = ""
            if len(sequences) >= num:
                break
        else:
            current_seq += line.strip()
            
    if current_seq and len(sequences) < num and len(current_seq) < MAX_LENGTH:
        sequences.append((current_header, current_seq))
        
    print(f"âœ… æˆåŠŸè·å– {len(sequences)} æ¡åºåˆ—ã€‚")
    return sequences

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹ {MODEL_NAME}...")
    model, alphabet = esm.pretrained.load_model_and_alphabet(MODEL_NAME)
    model.to(device)
    model.eval()
    batch_converter = alphabet.get_batch_converter()

    # 2. è·å–æ•°æ®
    data = fetch_membrane_proteins(NUM_SEQUENCES)
    
    # 3. è®¡ç®— Embeddings
    print("âš—ï¸ æ­£åœ¨è®¡ç®— Embeddings...")
    all_embeddings = []
    
    # é€æ¡å¤„ç† (Batch size = 1 æ¯”è¾ƒç¨³ï¼Œåæ­£å¾ˆå¿«)
    with torch.no_grad():
        for header, seq in tqdm(data):
            batch_labels, batch_strs, batch_tokens = batch_converter([(header, seq)])
            batch_tokens = batch_tokens.to(device)
            
            # ESM å‰å‘ä¼ æ’­
            # repr_layers=[33] è¡¨ç¤ºå–æœ€åä¸€å±‚ (650Mæ¨¡å‹å…±33å±‚)
            results = model(batch_tokens, repr_layers=[33], return_contacts=False)
            token_representations = results["representations"][33] # [1, L+2, D]
            
            # --- å…³é”®ç‚¹: é€‰æ‹© Pooling ç­–ç•¥ ---
            
            # ç­–ç•¥ A: [CLS] Token (åˆ†ç±»ä»»åŠ¡å¸¸ç”¨)
            # embedding = token_representations[0, 0] 
            
            # ç­–ç•¥ B: Mean Pooling (è¯­ä¹‰ç›¸ä¼¼åº¦å¸¸ç”¨ <- æ¨è!)
            # æ³¨æ„: æ’é™¤ <cls> (index 0) å’Œ <eos> (index -1)
            # batch_tokens ä¸­ padding çš„ä½ç½®ä¹Ÿè¦æ’é™¤ï¼Œä½†è¿™é‡Œ batch=1 æ‰€ä»¥ä¸ç”¨ç®¡ padding
            seq_len = len(seq)
            # å– 1 åˆ° seq_len+1 çš„èŒƒå›´ï¼Œé¿å¼€é¦–å°¾ç‰¹æ®Š token
            embedding = token_representations[0, 1 : seq_len + 1].mean(dim=0)
            
            all_embeddings.append(embedding)

    # 4. è®¡ç®—å¹³å‡å‘é‡
    if len(all_embeddings) == 0:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½• Embedding")
        return

    # Stack: [N, D] -> Mean: [D]
    all_embeddings_tensor = torch.stack(all_embeddings)
    target_embedding = all_embeddings_tensor.mean(dim=0)
    
    # å½’ä¸€åŒ– (è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼Œå› ä¸ºä¹‹åæˆ‘ä»¬è¦ç®— Cosine Similarity)
    target_embedding = target_embedding / target_embedding.norm(dim=-1, keepdim=True)
    
    # 5. ä¿å­˜
    # å¢åŠ ä¸€ä¸ªç»´åº¦å˜ä¸º [1, D]ï¼Œæ–¹ä¾¿åç»­çŸ©é˜µä¹˜æ³•
    target_embedding = target_embedding.unsqueeze(0) 
    
    torch.save(target_embedding, SAVE_PATH)
    print(f"ğŸ’¾ æˆåŠŸä¿å­˜è†œè›‹ç™½ç›®æ ‡å‘é‡è‡³: {os.path.abspath(SAVE_PATH)}")
    print(f"ğŸ“Š å‘é‡ç»´åº¦: {target_embedding.shape}")

if __name__ == "__main__":
    main()